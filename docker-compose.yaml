version: '3.6'

name: backstagecon-local-ai

services:

  local-ai:
    container_name: local-ai-api
    image: quay.io/go-skynet/local-ai:v1.30.0 # was: quay.io/go-skynet/local-ai:v1.25.0
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - 'THREADS=4' # Set this to the number of physical CPU's not threads
      - 'CONTEXT_SIZE=4096' # The historic memory for conversations
      - 'MODELS_PATH=/models' # Location of the models folder in the image
      - 'DEBUG=true' # More info for builds and output when tailing logs
      - 'GALLERIES=[{"name":"model-gallery", "url":"github:go-skynet/model-gallery/index.yaml"}, {"url": "github:go-skynet/model-gallery/huggingface.yaml","name":"huggingface"}]'
      - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/openllama-7b-open-instruct.yaml", "name": "openllama-7b"}, {"id": "model-gallery@mistral"}]' # Preload (auto-download) different models. See: https://github.com/go-skynet/model-gallery
    volumes:
      - ./localai-models:/models:cached # Maps the model folder to this project's models folder
    ports:
      - 8080:8080 # OpenAI compatible REST API will be on this port (outside:inside)
    command: ["/usr/bin/local-ai" ]

  chatbot-ui:
    container_name: chatbot-ui
    image: ghcr.io/mckaywrigley/chatbot-ui:main
    restart: unless-stopped
    environment:
      - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT="Below is an instruction that describes a task. Write a response that appropriately completes the request. Use markdown where appropriate."'
      - 'NEXT_PUBLIC_DEFAULT_TEMPERATURE="0.7"'
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX' # Not required by LocalAI etc.
      - 'OPENAI_API_HOST=http://local-ai-api:8080' # LocalAI API endpoint
      # - 'OPENAI_API_HOST=http://text-generation-webui:5001' # Alternative - Text Generation UI API server
      # - 'DEFAULT_MODEL=openllama-7b' # Specify the default model to use ("gpt-3.5-turbo" is default) doesn't tend to work anyway
    ports:
      - 3001:3000 # ChatBot UI will be on this port (outside:inside)

  text-generation-webui:
    image: atinoda/text-generation-webui:llama-cpu # Specify variant as the :tag (I'm using CPU only version - llama-cpu)
    container_name: text-generation-webui
    restart: unless-stopped
    environment:
        # Custom launch args (e.g., --model MODEL_NAME). 
        # If you haven't downloaded any models, remove --model MODEL or server won't boot.
      - EXTRA_LAUNCH_ARGS="--listen --verbose --api --extensions api openai" #  --model mistral-7b-openorca.Q4_K_M.gguf 
    ports:
      - 7860:7860  # Default GUI port (browser UI used for model settings and chat)
      - 5000:5000  # Default TGUI-API port (needs api extension running)
      - 5005:5005  # Default streaming port
      - 5001:5001  # Default OpenAI compatible API port (needs openai extension running - used by Chatbot UI)
    volumes:
      - ./text-generation-webui-config/chatlogs/Assistant:/app/logs/chat/Assistant
      - ./text-generation-webui-config/characters:/app/characters
      - ./text-generation-webui-config/loras:/app/loras
      - ./text-generation-webui-config/models:/app/models
      - ./text-generation-webui-config/presets:/app/presets
      - ./text-generation-webui-config/prompts:/app/prompts
      - ./text-generation-webui-config/training:/app/training
      - ./text-generation-webui-config/settings:/app/settings
      - ./text-generation-webui-config/extensions:/app/extensions  # Persist all extensions
      - type: bind
        source: ./text-generation-webui-config/settings.yaml
        target: /app/settings.yaml
        read_only: false
    logging:
      driver:  json-file
      options:
        max-file: "3"   # number of files or file count
        max-size: '10m'

  ollama-webui:
    container_name: ollama-webui
    image: ollamawebui/ollama-webui
    ports:
      - '3100:8080'
    environment:
      - 'OLLAMA_ORIGINS=*'
      - 'OLLAMA_HOST=0.0.0.0'

  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes:
      - './ollama:/root/.ollama'
    ports:
      - '11434:11434'

  big-agi:
    container_name: big-agi
    image: ghcr.io/enricoros/big-agi:main
    ports:
      - "3456:3000"
    command: [ "next", "start", "-p", "3000" ] 

  # chatgpt_web:
  #   container_name: chatgpt-web
  #   image: benwilcock/chatgpt-web
  #   restart: always
  #   ports:
  #     - 5173:5173
  #   volumes:
  #     - .:/app
  #   environment:
  #     - VITE_API_BASE=http://192.168.1.80:1234
  #     # - VITE_ENDPOINT_COMPLETIONS=/v1/chat/completions
  #     # - VITE_ENDPOINT_MODELS=/v1/models

  # openvino:
  #   container_name: openvino
  #   image: openvino/model_server:latest
  #   command: --model_name resnet --model_path /models/resnet50 --layout NHWC:NCHW --port 9000
  #   user: "${UID}:${GID}"
  #   volumes:
  #     - "./openvino/models:/models"
  #   ports:
  #     - "9000:9000"
